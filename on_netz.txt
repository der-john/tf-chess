This is just a notepad for my thoughts on writing netz - an adaptation of chessley-tan.


ISSUES / TOPICS
===============

2016-11-13:
___________

So, today I started writing a chess AI generator - the equivalent of train(er).py in chessley-tan's 2 systems. The two main points of reference (or of "educated pasting") are:

- elc1798/chessley-tan/tree/master/static/neural-net/train.py - it already contains a NN written with a little bit of tensorflow and lots of useful parts.

- ageron/handson-ml/blob/master/10_introduction_to_artificial_neural_networks.ipynb - it contains a very efficient textbook tensorflow NN that identifies digits.

I'm happy with my progress so far.


2016-11-14:
___________

Today I pushed my stuff to github and advanced on things I had begun yesterday. Now I've run into a big challenge:

chessley-tan's `trainer.py`, line 273: `gradients = tf.gradients(loss, params)`.

I've checked the values of the function call: They are `[None, None, None, None, None, None]`. I think this explains (see notes.txt, 1.) why the model doesn't get upgraded - these `None`s get converted to zeros and then the weights and biases don't get updated at all.

I'm fairly sure tensorflow can't calculate the gradients because the dependence of `loss` on `params` is "buried under too much code" and/or is not usual. (cf. https://stackoverflow.com/questions/43604608/why-is-my-tf-gradients-returning-none/43604747)

The initimidating thing is: calculating the gradients without this helper seems a little tough. One possible point of reference for this is here:

- https://www.coursera.org/learn/machine-learning/lecture/1z9WW/backpropagation-algorithm