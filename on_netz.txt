This is just a notepad for my thoughts on writing netz - an adaptation of chessley-tan.


ISSUES / TOPICS
===============

2016-11-13:
___________

So, today I started writing a chess AI generator - the equivalent of train(er).py in chessley-tan's 2 systems. The two main points of reference (or of "educated pasting") are:

- elc1798/chessley-tan/tree/master/static/neural-net/train.py - it already contains a NN written with a little bit of tensorflow and lots of useful parts.

- ageron/handson-ml/blob/master/10_introduction_to_artificial_neural_networks.ipynb - it contains a very efficient textbook tensorflow NN that identifies digits.

I'm happy with my progress so far.


2016-11-14:
___________

Today I pushed my stuff to github and advanced on things I had begun yesterday. Now I've run into a big challenge:

chessley-tan's `trainer.py`, line 273: `gradients = tf.gradients(loss, params)`.

I've checked the values of the function call: They are `[None, None, None, None, None, None]`. I think this explains (see notes.txt, 1.) why the model doesn't get upgraded - these `None`s get converted to zeros and then the weights and biases don't get updated at all.

I'm fairly sure tensorflow can't calculate the gradients because the dependence of `loss` on `params` is "buried under too much code" and/or is not usual. (cf. https://stackoverflow.com/questions/43604608/why-is-my-tf-gradients-returning-none/43604747)

What do I think is so unusual about our problem? I totally overlooked this fact at first. The loss = cost function depends on running the NN with three different input sets (X_child, X_rand, X_parent). I thought about "simplifying" this fact, by simply giving all layers another dimension with three entries, or multiplying the inputs by three. But that would be a totally different problem for the NN to solve. :(

So, as opposed to all other NN setups I've seen so far, we need to compute the outputs of one NN for three different input sets, then calculate the gradients for each weight and bias parameter using all of those, and then update the network. Really, even now, I'm not exactly sure what the calculation steps are!

One initimidating thing is: calculating the gradients without this helper seems a little tough. One possible point of reference for this is here:

- https://www.coursera.org/learn/machine-learning/lecture/1z9WW/backpropagation-algorithm